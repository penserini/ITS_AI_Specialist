{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c57fb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Versione compatibile con Google Colab con valori numerici e heatmap + visualizzazione Q, K, V\n",
    "# 📘 Visualizza la matrice di attenzione BERT con token ricostruiti e valori colorati\n",
    "\n",
    "# ⚙️ Installa pacchetti richiesti (Colab ha già tensorflow)\n",
    "!pip install -q transformers matplotlib\n",
    "\n",
    "# 📚 Import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "# 🔁 Caricamento modello BERT multilingua\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "# 🧠 Funzione per ricostruire i token originari\n",
    "def ricostruisci_parole(tokens):\n",
    "    parole = []\n",
    "    parola = \"\"\n",
    "    for tok in tokens:\n",
    "        if tok.startswith(\"##\"):\n",
    "            parola += tok[2:]\n",
    "        else:\n",
    "            if parola:\n",
    "                parole.append(parola)\n",
    "            parola = tok\n",
    "    if parola:\n",
    "        parole.append(parola)\n",
    "    return parole\n",
    "\n",
    "# 🔍 Visualizzazione matrice di attenzione con heatmap, numeri e Q/K/V\n",
    "def visualizza_attention_con_bert(frase):\n",
    "    inputs = tokenizer(frase, return_tensors=\"tf\")\n",
    "    outputs = model(**inputs, training=False)\n",
    "\n",
    "    # Ultimo layer, head 0\n",
    "    attention = outputs.attentions[-1][0][0].numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    parole = ricostruisci_parole(tokens)\n",
    "\n",
    "    print(\"\\nToken BERT:\", tokens)\n",
    "    print(\"Token ricostruiti:\", parole)\n",
    "\n",
    "    # Visualizza Q, K, V (solo se disponibili via output_hidden_states)\n",
    "    hidden_states = outputs.hidden_states[-1][0].numpy()  # shape: [seq_len, hidden_dim]\n",
    "    print(\"\\n🔹 Dimensione embedding finale:\", hidden_states.shape)\n",
    "\n",
    "    # Matrici Query, Key, Value simulate (solo per visualizzazione, non reali come nei pesi del modello)\n",
    "    Q = hidden_states @ np.random.rand(hidden_states.shape[1], hidden_states.shape[1])\n",
    "    K = hidden_states @ np.random.rand(hidden_states.shape[1], hidden_states.shape[1])\n",
    "    V = hidden_states @ np.random.rand(hidden_states.shape[1], hidden_states.shape[1])\n",
    "\n",
    "    print(\"\\n📌 Esempio dimensioni Q, K, V:\")\n",
    "    print(\"Q:\", Q.shape, \"\\nK:\", K.shape, \"\\nV:\", V.shape)\n",
    "\n",
    "    # Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    im = plt.imshow(attention, cmap=\"viridis\")\n",
    "    plt.title(\"Matrice di Attenzione BERT (Ultimo layer, Head 0)\")\n",
    "    plt.xlabel(\"Token osservato (Key)\")\n",
    "    plt.ylabel(\"Token osservatore (Query)\")\n",
    "    plt.xticks(np.arange(len(tokens)), tokens, rotation=45)\n",
    "    plt.yticks(np.arange(len(tokens)), tokens)\n",
    "    plt.colorbar(im, label=\"Peso di attenzione\")\n",
    "\n",
    "    # Numeri nella heatmap\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            val = attention[i, j]\n",
    "            plt.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if val < 0.5 else \"black\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ✏️ Inserisci la frase da analizzare qui:\n",
    "visualizza_attention_con_bert(\"Il gatto mangia il topo.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
