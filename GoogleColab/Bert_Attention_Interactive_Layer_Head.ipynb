{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c03130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Versione Colab con selezione interattiva layer/head + heatmap e QKV\n",
    "\n",
    "!pip install -q transformers matplotlib ipywidgets\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "model_name = \"bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name, output_attentions=True, output_hidden_states=True)\n",
    "\n",
    "def ricostruisci_parole(tokens):\n",
    "    parole = []\n",
    "    parola = \"\"\n",
    "    for tok in tokens:\n",
    "        if tok.startswith(\"##\"):\n",
    "            parola += tok[2:]\n",
    "        else:\n",
    "            if parola:\n",
    "                parole.append(parola)\n",
    "            parola = tok\n",
    "    if parola:\n",
    "        parole.append(parola)\n",
    "    return parole\n",
    "\n",
    "def visualizza_attention_interattiva(frase, layer=11, head=0):\n",
    "    inputs = tokenizer(frase, return_tensors=\"tf\")\n",
    "    outputs = model(**inputs, training=False)\n",
    "\n",
    "    attention = outputs.attentions[layer][0][head].numpy()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    parole = ricostruisci_parole(tokens)\n",
    "\n",
    "    print(\"\\nToken BERT:\", tokens)\n",
    "    print(\"Token ricostruiti:\", parole)\n",
    "\n",
    "    hidden_states = outputs.hidden_states[layer][0].numpy()\n",
    "    print(\"\\nðŸ”¹ Embedding shape (layer {}):\".format(layer), hidden_states.shape)\n",
    "\n",
    "    Q = hidden_states @ np.random.rand(hidden_states.shape[1], hidden_states.shape[1])\n",
    "    K = hidden_states @ np.random.rand(hidden_states.shape[1], hidden_states.shape[1])\n",
    "    V = hidden_states @ np.random.rand(hidden_states.shape[1], hidden_states.shape[1])\n",
    "\n",
    "    print(\"ðŸ“Œ Q:\", Q.shape, \", K:\", K.shape, \", V:\", V.shape)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    im = plt.imshow(attention, cmap=\"viridis\")\n",
    "    plt.title(f\"Matrice di Attenzione (Layer {layer}, Head {head})\")\n",
    "    plt.xlabel(\"Token osservato (Key)\")\n",
    "    plt.ylabel(\"Token osservatore (Query)\")\n",
    "    plt.xticks(np.arange(len(tokens)), tokens, rotation=45)\n",
    "    plt.yticks(np.arange(len(tokens)), tokens)\n",
    "    plt.colorbar(im, label=\"Peso di attenzione\")\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(len(tokens)):\n",
    "            val = attention[i, j]\n",
    "            plt.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\",\n",
    "                     color=\"white\" if val < 0.5 else \"black\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ðŸ”˜ Interfaccia interattiva\n",
    "interact(\n",
    "    visualizza_attention_interattiva,\n",
    "    frase=\"Il gatto mangia il topo.\",\n",
    "    layer=IntSlider(min=0, max=11, step=1, value=11, description='Layer'),\n",
    "    head=IntSlider(min=0, max=11, step=1, value=0, description='Head')\n",
    ");\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
